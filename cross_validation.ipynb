{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NczvF1LNy0-J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: '!pip'\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting keras-tuner\n",
      "  Using cached keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
      "Collecting kt-legacy\n",
      "  Using cached kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (7.33.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (2.27.1)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (59.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (3.0.29)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (5.2.1.post0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2.0.12)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (2.6.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (1.46.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (3.3.7)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
      "Installing collected packages: tensorboard-plugin-wit, kt-legacy, werkzeug, tensorboard-data-server, protobuf, absl-py, google-auth-oauthlib, tensorboard, keras-tuner\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'message.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting print_schema\n",
      "  Using cached print_schema-1.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from print_schema) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (2.0.12)\n",
      "Installing collected packages: print_schema\n",
      "Successfully installed print_schema-1.1.1\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Collecting opensmile\n",
      "  Using cached opensmile-2.4.1-py3-none-any.whl (4.5 MB)\n",
      "Collecting audinterface>=0.7.0\n",
      "  Using cached audinterface-0.9.0-py3-none-any.whl (29 kB)\n",
      "Collecting audobject>=0.6.1\n",
      "  Using cached audobject-0.7.2-py3-none-any.whl (23 kB)\n",
      "Collecting audformat<2.0.0,>=0.12.1\n",
      "  Using cached audformat-0.14.3-py3-none-any.whl (48 kB)\n",
      "Collecting audresample<2.0.0,>=1.1.0\n",
      "  Using cached audresample-1.1.0-py3-none-any.whl (635 kB)\n",
      "Collecting oyaml\n",
      "  Using cached oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Collecting audeer>=1.18.0\n",
      "  Using cached audeer-1.18.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from audobject>=0.6.1->opensmile) (4.11.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from audeer>=1.18.0->audobject>=0.6.1->opensmile) (4.64.0)\n",
      "Collecting iso-639\n",
      "  Using cached iso_639-0.4.5-py3-none-any.whl\n",
      "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in /opt/conda/lib/python3.7/site-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.3.5)\n",
      "Collecting audiofile>=0.4.0\n",
      "  Using cached audiofile-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting iso3166\n",
      "  Using cached iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.7/site-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from audresample<2.0.0,>=1.1.0->audinterface>=0.7.0->opensmile) (1.19.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (4.2.0)\n",
      "Collecting sox\n",
      "  Using cached sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
      "Collecting soundfile\n",
      "  Using cached SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.21)\n",
      "Installing collected packages: iso-639, sox, oyaml, iso3166, audresample, audeer, soundfile, audobject, audiofile, audformat, audinterface, opensmile\n",
      "Successfully installed audeer-1.18.0 audformat-0.14.3 audinterface-0.9.0 audiofile-1.1.0 audobject-0.7.2 audresample-1.1.0 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.4.1 oyaml-1.0 soundfile-0.10.3.post1 sox-1.4.1\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.0)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.24.32-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.27.1)\n",
      "Collecting regex\n",
      "  Using cached regex-2022.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting botocore<1.28.0,>=1.27.32\n",
      "  Using cached botocore-1.27.32-py3-none-any.whl (9.0 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.32->boto3) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.8.0)\n",
      "Installing collected packages: sentencepiece, regex, jmespath, botocore, sacremoses, s3transfer, boto3\n",
      "Successfully installed boto3-1.24.32 botocore-1.27.32 jmespath-1.0.1 regex-2022.7.9 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.96\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.1 huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install !pip install tensorflow_io\n",
    "!pip install keras-tuner\n",
    "!pip install print_schema\n",
    "!pip install pydub\n",
    "!pip install opensmile\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2csKkek1vzn",
    "outputId": "bb4019bd-eeb1-4d36-de2e-48db5ade5fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hDOQBXGGrB28"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F9IYaisF3Tuv"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "id = data.iloc[:,0].values\n",
    "train_labels = data.iloc[:,1].values\n",
    "train_texts = data.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IgrPKwN8zdk9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "df[\"Content\"].fillna(\"\",inplace=True)\n",
    "test_texts = list(df.loc[:,\"Content\"])\n",
    "test_labels = list(df.loc[:,\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mwPLzHEh7x2v"
   },
   "outputs": [],
   "source": [
    "def find_pos_sample_frac(sample_label):\n",
    "  return (sample_label == 1).sum() / sample_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kUrQvrbx3ToM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def get_several_validation_set(content, label, val_fraction=0.2, total_splits=5, seed=0):\n",
    "  sss = StratifiedShuffleSplit(n_splits=total_splits, test_size=val_fraction, random_state=seed)\n",
    "  return sss.split(content, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QXbM_NJjAMCr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "np.random.seed(42)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faw1p0Q7H_je"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FwPC_CXF-nSd"
   },
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=20):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    " \n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    " \n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    " \n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    " \n",
    "    # ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "    #            c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    " \n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_splits+1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+1.1, -.1], xlim=[0, 166])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "SNaAqR16-Z0A",
    "outputId": "8e935920-ee12-4abe-e1df-f146ab8784f7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/2467293762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_cv_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfile_keys_sorted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_cv_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_keys_sorted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_keys_sorted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# groups = np.hstack([[ii] * 16 for ii in range(10)])\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "cv = StratifiedShuffleSplit(n_splits=4, test_size=0.2)\n",
    "ax1 = plot_cv_indices(cv, content, label, ax1, 4)\n",
    "file_keys_sorted_index = np.argsort(label)\n",
    "ax2 = plot_cv_indices(cv, content[file_keys_sorted_index], label[file_keys_sorted_index], ax2, 4)\n",
    "ax1.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(0, .95))\n",
    "ax2.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(0, .95))\n",
    "# Make the legend fit\n",
    "# plt.tight_layout()\n",
    "fig.subplots_adjust()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aUaMto4-aAz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUzgPvUguSt_"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3NPEdNAunTa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bYM53WOuX82",
    "outputId": "2e419047-9c94-4a66-8db1-2318b285e580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using cache found in /home/jupyter/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P77XZHZKuX5U"
   },
   "outputs": [],
   "source": [
    "class ADdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JGvFuNl7uX0_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 256, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(D_in, 256),\n",
    "            nn.BatchNorm1d(num_features=256), ## kk\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            #nn.BatchNorm1d(num_features=128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, 1)\n",
    "            \n",
    "            #nn.BatchNorm1d(num_features=128), ###\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Dropout(0.5),\n",
    "\n",
    "            #nn.Linear(H, D_out)\n",
    "            #nn.Linear(128, 2)\n",
    "            #nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Bf2CJWiSuXvd"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def initialize_model(epochs=50):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ulMFo0Rzu2Jn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn =  torch.nn.BCEWithLogitsLoss()\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_loader, optim, val_loader=None, epochs=50, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = [] \n",
    "    for epoch_i in range(epochs):\n",
    "        train_loss_sum = 0\n",
    "        train_accuracy_epoch = 0\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for batch in (train_loader):\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            logits = logits.reshape(-1) #silebilirsin\n",
    "            \n",
    "            loss = loss_fn(logits, labels.float())\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "            logits_class = logits > 0.5\n",
    "            train_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "            train_accuracy_epoch += train_acc\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = np.round(train_loss_sum/len(train_loader),2)\n",
    "        avg_train_acc = np.round(train_accuracy_epoch/len(train_loader),2)\n",
    "\n",
    "        if evaluation == True:\n",
    "            avg_val_loss, avg_val_acc = evaluate(model, val_loader)\n",
    "        # print('Epoch {}, train loss {} , val loss is {}, train acc is {}, val acc is {} '.format(epoch_i,avg_train_loss,avg_val_loss,avg_train_acc,avg_val_acc))\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model,train_loss_list,val_loss_list,train_acc_list,val_acc_list\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_loss_sum = 0\n",
    "    val_accuracy_epoch = 0\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        val_loss_sum +=loss.item()\n",
    "        avg_val_loss = np.round(val_loss_sum/len(val_dataloader),2)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        val_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        val_accuracy_epoch += val_acc\n",
    "        avg_val_acc = np.round(val_accuracy_epoch/len(val_dataloader),2)\n",
    "\n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "\n",
    "def evaluate_test(model, test_dataloader):\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        test_loss_sum +=loss.item()\n",
    "        avg_test_loss = np.round(test_loss_sum/len(test_dataloader),5)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        predictions.append(logits.cpu())\n",
    "        test_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        test_accuracy_epoch += test_acc\n",
    "        avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_loss, avg_test_acc, predictions, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_B21rPpv0Xf"
   },
   "source": [
    "### Train alone to vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oc4D0_et3Thm",
    "outputId": "e9091317-c7ef-427a-ede7-9746e3649128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new set: \n",
      "Val set size  0.20481927710843373\n",
      "Pos sample %:  0.5294117647058824\n",
      "     TRAIN: [160 146  51 130   3] TEST: [105 136  20  36 121]\n",
      "new set: \n",
      "Val set size  0.20481927710843373\n",
      "Pos sample %:  0.5294117647058824\n",
      "     TRAIN: [ 92  11 119 155  50] TEST: [ 58  93  23 147   0]\n",
      "new set: \n",
      "Val set size  0.20481927710843373\n",
      "Pos sample %:  0.5294117647058824\n",
      "     TRAIN: [ 95  25  61  18 113] TEST: [  2  22 117  10  23]\n",
      "new set: \n",
      "Val set size  0.20481927710843373\n",
      "Pos sample %:  0.5294117647058824\n",
      "     TRAIN: [ 55  80  52  70 126] TEST: [146 155  17 149  23]\n",
      "new set: \n",
      "Val set size  0.20481927710843373\n",
      "Pos sample %:  0.5294117647058824\n",
      "     TRAIN: [152 133 129  23 106] TEST: [ 44 121 101  29  85]\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in train_and_val_splits:\n",
    "    print(\"new set: \")\n",
    "    label_val_set = train_labels[val_index]\n",
    "    print(\"Val set size \", val_index.shape[0] / train_labels.shape[0])\n",
    "    print(\"Pos sample %: \", find_pos_sample_frac(label_val_set))\n",
    "    print(\" \" * 5 + \"TRAIN:\", train_index[:5], \"TEST:\", val_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dOLZFsYcu3Q7"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "test_dataset = ADdataset(test_encodings, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UyCM-Y4-u7QV"
   },
   "outputs": [],
   "source": [
    "# seeds = list(range(0,60,6))\n",
    "mean_val_acc = {}\n",
    "last_val_acc = {}\n",
    "test_acc = {}\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmAxPBGAWRjC"
   },
   "source": [
    "**Different seed || Multiple epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-cZ2FoDTJrK"
   },
   "source": [
    "**Single seed || Multiple epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, val_index in train_and_val_splits:\n",
    "    trains.append(train_index)\n",
    "    vals.append(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ipDH4sDXu7KB",
    "outputId": "c0b03e9a-d080-48b6-aa1c-21df8d6c0c9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.53905, Test Accuracy: 0.84524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.87953, Test Accuracy: 0.80159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.60234, Test Accuracy: 0.81548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.59169, Test Accuracy: 0.84524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.42473, Test Accuracy: 0.87302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.5463, Test Accuracy: 0.85913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.75432, Test Accuracy: 0.81746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.60126, Test Accuracy: 0.82937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 14.76 GiB total capacity; 13.41 GiB already allocated; 41.75 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/4000567438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmax_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1/3406052917.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optim, val_loader, epochs, evaluation)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Perform a forward pass. This will return logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Compute loss and accumulate the loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1/1944328712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Feed input to BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         outputs = self.bert(input_ids=input_ids,\n\u001b[0;32m---> 60\u001b[0;31m                             attention_mask=attention_mask)\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Extract the last hidden state of the token `[CLS]` for classification task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 14.76 GiB total capacity; 13.41 GiB already allocated; 41.75 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "seed = 42\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "mean_val_acc[seed] = []\n",
    "last_val_acc[seed] = []\n",
    "test_acc[seed] = []\n",
    "models = []\n",
    "index = 1\n",
    "train_and_val_splits = get_several_validation_set(train_texts, train_labels, total_splits=20, seed=seed)\n",
    "for train_index, val_index in train_and_val_splits:\n",
    "    gc.collect()\n",
    "    train_encodings = tokenizer(list(train_texts[train_index]), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(train_texts[val_index]), truncation=True, padding=True)\n",
    "    train_dataset = ADdataset(train_encodings, train_labels[train_index])\n",
    "    val_dataset = ADdataset(val_encodings, train_labels[val_index])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n",
    "    bert_classifier,train_loss_list,val_loss_list,train_acc_list,val_acc_list = train(bert_classifier, train_loader, optimizer, val_loader, epochs=20, evaluation=True)\n",
    "\n",
    "    max_val_acc = max(val_acc_list)\n",
    "    if max_val_acc > best_val_acc:\n",
    "        best_model_val = bert_classifier\n",
    "        torch.save(best_model_val, \"weights/bestModel_val.pt\")\n",
    "\n",
    "\n",
    "    test_loss, test_accuracy, predictions, labels_l = evaluate_test(bert_classifier, test_loader)\n",
    "    print(\"Test Loss: {}, Test Accuracy: {}\".format(test_loss,test_accuracy))\n",
    "\n",
    "    if test_accuracy > best_test_acc:\n",
    "        best_model_test = bert_classifier\n",
    "        torch.save(best_model_test, \"weights/bestModel_test.pt\")\n",
    "    torch.save(bert_classifier, f\"weights/Model_{index}.pt\")\n",
    "    models.append(bert_classifier)\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "    mean_val_acc[seed].append(np.mean(val_acc_list))\n",
    "    last_val_acc[seed].append(val_acc_list[-1])\n",
    "    test_acc[seed].append(test_accuracy)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "Test Loss: 0.58624, Test Accuracy: 0.84524\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "seed = 42\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# mean_val_acc[seed] = []\n",
    "# last_val_acc[seed] = []\n",
    "# test_acc[seed] = []\n",
    "# models = []\n",
    "index = 1\n",
    "# train_and_val_splits = get_several_validation_set(train_texts, train_labels, total_splits=20, seed=seed)\n",
    "for train_index, val_index in zip(trains, vals):\n",
    "    if index < 11:\n",
    "        index += 1\n",
    "        continue\n",
    "    gc.collect()\n",
    "    train_encodings = tokenizer(list(train_texts[train_index]), truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(list(train_texts[val_index]), truncation=True, padding=True)\n",
    "    train_dataset = ADdataset(train_encodings, train_labels[train_index])\n",
    "    val_dataset = ADdataset(val_encodings, train_labels[val_index])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n",
    "    bert_classifier,train_loss_list,val_loss_list,train_acc_list,val_acc_list = train(bert_classifier, train_loader, optimizer, val_loader, epochs=20, evaluation=True)\n",
    "\n",
    "    max_val_acc = max(val_acc_list)\n",
    "    # if max_val_acc > best_val_acc:\n",
    "    #     best_model_val = bert_classifier\n",
    "    #     torch.save(best_model_val, \"weights/bestModel_val.pt\")\n",
    "\n",
    "\n",
    "    test_loss, test_accuracy, predictions, labels_l = evaluate_test(bert_classifier, test_loader)\n",
    "    print(\"Test Loss: {}, Test Accuracy: {}\".format(test_loss,test_accuracy))\n",
    "\n",
    "    # if test_accuracy > best_test_acc:\n",
    "    #     best_model_test = bert_classifier\n",
    "    #     torch.save(best_model_test, \"weights/bestModel_test.pt\")\n",
    "    torch.save(bert_classifier, f\"weights/Model_{index}.pt\")\n",
    "    # models.append(bert_classifier)\n",
    "    index += 1\n",
    "    \n",
    "\n",
    "    mean_val_acc[seed].append(np.mean(val_acc_list))\n",
    "    last_val_acc[seed].append(val_acc_list[-1])\n",
    "    test_acc[seed].append(test_accuracy)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{42: [0.8564999999999999, 0.6935, 0.8185, 0.8730000000000002, 0.8805000000000002, 0.79, 0.7310000000000001, 0.7304999999999999, 0.7374999999999999, 0.8905, 0.8400000000000001]}\n",
      "{42: [0.9, 0.78, 0.88, 0.92, 0.92, 0.75, 0.82, 0.75, 0.78, 0.95, 0.82]}\n",
      "{42: [0.84524, 0.80159, 0.81548, 0.84524, 0.87302, 0.85913, 0.81746, 0.82937, 0.83135, 0.81746, 0.84524]}\n"
     ]
    }
   ],
   "source": [
    "print(mean_val_acc)\n",
    "print(last_val_acc)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zrsXjDaDu7CB"
   },
   "outputs": [],
   "source": [
    "def evaluate_ensemble(models, test_dataloader):\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "\n",
    "      prediction = []\n",
    "      for model in models:\n",
    "        model.eval()\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "          logits = model(input_ids, attention_mask)\n",
    "          labels_list.append(labels.cpu())\n",
    "\n",
    "          # Compute loss\n",
    "          logits = logits.reshape(-1)\n",
    "          # loss = loss_fn(logits, labels.float())\n",
    "          # test_loss_sum +=loss.item()\n",
    "          # avg_test_loss = np.round(test_loss_sum/len(test_dataloader),5)\n",
    "\n",
    "          logits_class = logits > 0.5\n",
    "          prediction.append(logits_class)\n",
    "\n",
    "      prediction_ensemble = sum(prediction) > 0.5*len(prediction)\n",
    "      predictions.append(prediction_ensemble.cpu())\n",
    "\n",
    "      test_acc = (labels == prediction_ensemble).sum().item() / labels.size(0)\n",
    "      test_accuracy_epoch += test_acc\n",
    "      avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_acc, predictions, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{42: [0.8564999999999999,\n",
       "  0.6935,\n",
       "  0.8185,\n",
       "  0.8730000000000002,\n",
       "  0.8805000000000002,\n",
       "  0.79,\n",
       "  0.7310000000000001,\n",
       "  0.7304999999999999,\n",
       "  0.7374999999999999,\n",
       "  0.8905,\n",
       "  0.8400000000000001],\n",
       " 0: [0.7885, 0.7945000000000001, 0.6014999999999999, 0.7679999999999999],\n",
       " 10: [0.7704999999999999, 0.762, 0.8005000000000001, 0.7195],\n",
       " 20: [0.7934999999999999, 0.7675000000000001, 0.8285000000000002, 0.7865],\n",
       " 30: [0.7394999999999998, 0.8100000000000002, 0.7344999999999999, 0.7435],\n",
       " 40: [0.7825, 0.8615, 0.7929999999999999, 0.7695000000000001]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=50    index=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=50    index=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=50    index=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=50    index=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=60    index=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=60    index=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=60    index=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n",
      "seed=60    index=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for seed in range(50,70,10):\n",
    "    set_seed(seed)    # Set seed for reproducibility\n",
    "    seed = seed\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    mean_val_acc[seed] = []\n",
    "    last_val_acc[seed] = []\n",
    "    test_acc[seed] = []\n",
    "    # models = []\n",
    "    index = 1\n",
    "    train_and_val_splits = get_several_validation_set(train_texts, train_labels, total_splits=4, seed=seed)\n",
    "    for train_index, val_index in train_and_val_splits:\n",
    "        print(f\"seed={seed}    index={index}\")\n",
    "        gc.collect()\n",
    "        train_encodings = tokenizer(list(train_texts[train_index]), truncation=True, padding=True)\n",
    "        val_encodings = tokenizer(list(train_texts[val_index]), truncation=True, padding=True)\n",
    "        train_dataset = ADdataset(train_encodings, train_labels[train_index])\n",
    "        val_dataset = ADdataset(val_encodings, train_labels[val_index])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n",
    "        bert_classifier,train_loss_list,val_loss_list,train_acc_list,val_acc_list = train(bert_classifier, train_loader, optimizer, val_loader, epochs=20, evaluation=True)\n",
    "\n",
    "        # max_val_acc = max(val_acc_list)\n",
    "        # if max_val_acc > best_val_acc:\n",
    "        #     best_model_val = bert_classifier\n",
    "        #     torch.save(best_model_val, \"weights/bestModel_val.pt\")\n",
    "\n",
    "\n",
    "        test_loss, test_accuracy, predictions, labels_l = evaluate_test(bert_classifier, test_loader)\n",
    "        # print(\"Test Loss: {}, Test Accuracy: {}\".format(test_loss,test_accuracy))\n",
    "\n",
    "        # if test_accuracy > best_test_acc:\n",
    "        #     best_model_test = bert_classifier\n",
    "        #     torch.save(best_model_test, \"weights/bestModel_test.pt\")\n",
    "        torch.save(bert_classifier, f\"different_seed/Model_{seed}_{index}.pt\")\n",
    "        # models.append(bert_classifier)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "        mean_val_acc[seed].append(np.mean(val_acc_list))\n",
    "        last_val_acc[seed].append(val_acc_list[-1])\n",
    "        test_acc[seed].append(test_accuracy)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "del test_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob(\"different_seed/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [02:46<00:00,  5.95s/it]\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "for filename in tqdm(filenames):\n",
    "    test_models.append(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:30<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# val_loss = []\n",
    "# val_acc = []\n",
    "test_los = []\n",
    "test_acc = []\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "for test_model in tqdm(test_models):\n",
    "    # avg_val_loss, avg_val_acc = evaluate(test_model, test_loader)\n",
    "    test_loss, test_accuracy, predictions, labels_l = evaluate_test(test_model, test_loader)\n",
    "    # val_loss.append(avg_val_loss)\n",
    "    # val_acc.append(avg_val_acc)\n",
    "    test_los.append(test_loss)\n",
    "    test_acc.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(font_scale=1,style=\"whitegrid\",rc={'figure.dpi':100})\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 4))\n",
    "ax.set_title(\"Test accuracy of different seeds with cross validation\")\n",
    "ax = sns.boxplot(x=test_acc)\n",
    "ax = sns.swarmplot(x=test_acc, color=\".2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_ensemble, predictions_ensemble, labels_l_ensemble = evaluate_ensemble(test_models, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85536"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy_ensemble"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cross validation.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
