{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1124b28b-8694-4a00-af05-00d0c98a3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-1.8.0-py2.py3-none-any.whl (153 kB)\n",
      "Collecting pathtools\n",
      "  Using cached pathtools-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n",
      "Installing collected packages: pathtools, shortuuid, setproctitle, sentry-sdk, promise, wandb\n",
      "Successfully installed pathtools-0.1.2 promise-2.3 sentry-sdk-1.8.0 setproctitle-1.2.3 shortuuid-1.0.9 wandb-0.12.21\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9515f3fe-a7d0-4233-b581-fda8530beec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a5e4cb-40c3-4764-9ae6-7f115c552982",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'relogin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/806510960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'relogin'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.relogin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0e7121-9750-4301-b63b-1ba7b80078d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/641029006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bert-cross_validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"miraclelinzzz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Bert-cross_validation\", entity=\"miraclelinzzz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1a24a6-06eb-47b1-9ea7-fef3805cd175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8646e8-5ecc-4859-bd6c-68188f1dcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "id = data.iloc[:,0].values\n",
    "train_labels = data.iloc[:,1].values\n",
    "train_texts = data.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ef6587-9054-436f-9c0b-60adabfa8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "df[\"Content\"].fillna(\"\",inplace=True)\n",
    "test_texts = list(df.loc[:,\"Content\"])\n",
    "test_labels = list(df.loc[:,\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806360be-3015-4bd0-b7ce-6932496730a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def get_several_validation_set(content, label, val_fraction=0.2, total_splits=5, seed=0):\n",
    "  sss = StratifiedShuffleSplit(n_splits=total_splits, test_size=val_fraction, random_state=seed)\n",
    "  return sss.split(content, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79f6969-4914-40d0-9815-8ef0a880b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9246b8-1ff3-4627-8e89-bfc9cd89b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba77801-3d4d-4120-8378-76e7b81d0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd50db2-8ea4-479b-8906-2b9774f8f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "class ADdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 256, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(D_in, 256),\n",
    "            nn.BatchNorm1d(num_features=256), ## kk\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            #nn.BatchNorm1d(num_features=128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, 1)\n",
    "            \n",
    "            #nn.BatchNorm1d(num_features=128), ###\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Dropout(0.5),\n",
    "\n",
    "            #nn.Linear(H, D_out)\n",
    "            #nn.Linear(128, 2)\n",
    "            #nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "def initialize_model(epochs=50):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "# Specify loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn =  torch.nn.BCEWithLogitsLoss()\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_loader, optim, val_loader=None, epochs=50, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = [] \n",
    "    for epoch_i in range(epochs):\n",
    "        train_loss_sum = 0\n",
    "        train_accuracy_epoch = 0\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for batch in (train_loader):\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            logits = logits.reshape(-1) #silebilirsin\n",
    "            \n",
    "            loss = loss_fn(logits, labels.float())\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "            logits_class = logits > 0.5\n",
    "            train_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "            train_accuracy_epoch += train_acc\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = np.round(train_loss_sum/len(train_loader),2)\n",
    "        avg_train_acc = np.round(train_accuracy_epoch/len(train_loader),2)\n",
    "\n",
    "        if evaluation == True:\n",
    "            avg_val_loss, avg_val_acc = evaluate(model, val_loader)\n",
    "        print('Epoch {}, train loss {} , val loss is {}, train acc is {}, val acc is {} '.format(epoch_i,avg_train_loss,avg_val_loss,avg_train_acc,avg_val_acc))\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "        wandb.log({\"train_loss\": avg_train_loss, \"train_acc\": avg_train_acc, \"val_loss\": avg_val_loss, \"val_acc\": avg_val_acc})\n",
    "    print(\"Training complete!\")\n",
    "    return model,train_loss_list,val_loss_list,train_acc_list,val_acc_list\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_loss_sum = 0\n",
    "    val_accuracy_epoch = 0\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        val_loss_sum +=loss.item()\n",
    "        avg_val_loss = np.round(val_loss_sum/len(val_dataloader),2)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        val_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        val_accuracy_epoch += val_acc\n",
    "        avg_val_acc = np.round(val_accuracy_epoch/len(val_dataloader),2)\n",
    "\n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "\n",
    "def evaluate_test(model, test_dataloader):\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        test_loss_sum +=loss.item()\n",
    "        avg_test_loss = np.round(test_loss_sum/len(test_dataloader),5)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        predictions.append(logits.cpu())\n",
    "        test_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        test_accuracy_epoch += test_acc\n",
    "        avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_loss, avg_test_acc, predictions, labels_list\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, test_dataloader):\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "\n",
    "      prediction = []\n",
    "      for model in models:\n",
    "        model.eval()\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "          logits = model(input_ids, attention_mask)\n",
    "          labels_list.append(labels.cpu())\n",
    "\n",
    "          logits = logits.reshape(-1)\n",
    "          logits_class = logits > 0.5\n",
    "          prediction.append(logits_class)\n",
    "\n",
    "      prediction_ensemble = sum(prediction) > 0.5*len(prediction)\n",
    "      predictions.append(prediction_ensemble.cpu())\n",
    "\n",
    "      test_acc = (labels == prediction_ensemble).sum().item() / labels.size(0)\n",
    "      test_accuracy_epoch += test_acc\n",
    "      avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_acc, predictions, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14af6ac-231a-40c1-961c-458feb3aa87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "test_dataset = ADdataset(test_encodings, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2704dda-6b28-4256-9fdb-3dd43ec65da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_splits = get_several_validation_set(train_texts, train_labels, total_splits=20, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760ffc5-84ee-4dc7-bc2f-054a0ed25c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_and_val_splits:\n",
    "    train_index = i\n",
    "    val_index = j\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d3972-3b30-4a82-9cf6-68c4de9c700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Bert-cross_validation\", entity=\"miraclelinzzz\")\n",
    "gc.collect()\n",
    "train_encodings = tokenizer(list(train_texts[train_index]), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(train_texts[val_index]), truncation=True, padding=True)\n",
    "\n",
    "train_dataset = ADdataset(train_encodings, train_labels[train_index])\n",
    "val_dataset = ADdataset(val_encodings, train_labels[val_index])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for _ in range(5):\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n",
    "    bert_classifier,train_loss_list,val_loss_list,train_acc_list,val_acc_list = train(bert_classifier, train_loader, optimizer, val_loader, epochs=20, evaluation=True)\n",
    "\n",
    "    # max_val_acc = max(val_acc_list)\n",
    "    # if max_val_acc > best_val_acc:\n",
    "    #     best_model_val = bert_classifier\n",
    "    #     torch.save(best_model_val, \"weights/bestModel_val.pt\")\n",
    "\n",
    "\n",
    "    test_loss, test_accuracy, predictions, labels_l = evaluate_test(bert_classifier, test_loader)\n",
    "    print(\"Test Loss: {}, Test Accuracy: {}\".format(test_loss,test_accuracy))\n",
    "    wandb.summary['test_losss'] = test_loss\n",
    "    wandb.summary['test_accuracy'] = test_accuracy\n",
    "    # if test_accuracy > best_test_acc:\n",
    "    #     best_model_test = bert_classifier\n",
    "    #     torch.save(best_model_test, \"weights/bestModel_test.pt\")\n",
    "    # torch.save(bert_classifier, f\"weights/Model_{index}.pt\")\n",
    "    # models.append(bert_classifier)\n",
    "    # index += 1\n",
    "\n",
    "\n",
    "    # mean_val_acc[seed].append(np.mean(val_acc_list))\n",
    "    # last_val_acc[seed].append(val_acc_list[-1])\n",
    "    # test_acc[seed].append(test_accuracy)\n",
    "    gc.collect()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ad396-ba44-4aa4-b44b-6caab339458e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
