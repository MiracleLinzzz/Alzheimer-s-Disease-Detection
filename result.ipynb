{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da25029-576f-441b-a5c2-4ce4a6847827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_io\n",
      "  Using cached tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.26.0\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow_io\n",
      "Successfully installed tensorflow-io-gcs-filesystem-0.26.0 tensorflow_io-0.26.0\n",
      "Collecting keras-tuner\n",
      "  Using cached keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
      "Collecting kt-legacy\n",
      "  Using cached kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (21.3)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (7.33.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (3.0.30)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (59.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner) (5.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2022.6.15)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (3.4.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner) (1.48.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard->keras-tuner) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
      "Installing collected packages: tensorboard-plugin-wit, kt-legacy, werkzeug, tensorboard-data-server, protobuf, absl-py, google-auth-oauthlib, tensorboard, keras-tuner\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'message_factory.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting print_schema\n",
      "  Using cached print_schema-1.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from print_schema) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->print_schema) (3.3)\n",
      "Installing collected packages: print_schema\n",
      "Successfully installed print_schema-1.1.1\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Collecting opensmile\n",
      "  Using cached opensmile-2.4.1-py3-none-any.whl (4.5 MB)\n",
      "Collecting audinterface>=0.7.0\n",
      "  Using cached audinterface-0.9.1-py3-none-any.whl (30 kB)\n",
      "Collecting audobject>=0.6.1\n",
      "  Using cached audobject-0.7.5-py3-none-any.whl (24 kB)\n",
      "Collecting audformat<2.0.0,>=0.12.1\n",
      "  Using cached audformat-0.14.3-py3-none-any.whl (48 kB)\n",
      "Collecting audresample<2.0.0,>=1.1.0\n",
      "  Using cached audresample-1.1.0-py3-none-any.whl (635 kB)\n",
      "Collecting audeer>=1.18.0\n",
      "  Using cached audeer-1.18.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from audobject>=0.6.1->opensmile) (4.11.4)\n",
      "Collecting oyaml\n",
      "  Using cached oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from audeer>=1.18.0->audobject>=0.6.1->opensmile) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.7/site-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (6.0)\n",
      "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in /opt/conda/lib/python3.7/site-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.3.5)\n",
      "Collecting iso-639\n",
      "  Using cached iso_639-0.4.5-py3-none-any.whl\n",
      "Collecting iso3166\n",
      "  Using cached iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
      "Collecting audiofile>=0.4.0\n",
      "  Using cached audiofile-1.1.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from audresample<2.0.0,>=1.1.0->audinterface>=0.7.0->opensmile) (1.19.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (4.3.0)\n",
      "Collecting soundfile\n",
      "  Using cached SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting sox\n",
      "  Using cached sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.21)\n",
      "Installing collected packages: iso-639, sox, oyaml, iso3166, audresample, audeer, soundfile, audobject, audiofile, audformat, audinterface, opensmile\n",
      "Successfully installed audeer-1.18.0 audformat-0.14.3 audinterface-0.9.1 audiofile-1.1.0 audobject-0.7.5 audresample-1.1.0 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.4.1 oyaml-1.0 soundfile-0.10.3.post1 sox-1.4.1\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.0)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.24.54-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.28.1)\n",
      "Collecting regex\n",
      "  Using cached regex-2022.8.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (752 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.54\n",
      "  Using cached botocore-1.27.54-py3-none-any.whl (9.1 MB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.54->boto3) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.8.1)\n",
      "Installing collected packages: sentencepiece, regex, jmespath, botocore, sacremoses, s3transfer, boto3\n",
      "Successfully installed boto3-1.24.54 botocore-1.27.54 jmespath-1.0.1 regex-2022.8.17 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.8.0 huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting pathtools\n",
      "  Using cached pathtools-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.1)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\n",
      "Collecting promise<3,>=2.0\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.3.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.1)\n",
      "Installing collected packages: pathtools, shortuuid, setproctitle, sentry-sdk, promise, wandb\n",
      "Successfully installed pathtools-0.1.2 promise-2.3 sentry-sdk-1.9.5 setproctitle-1.3.2 shortuuid-1.0.9 wandb-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_io\n",
    "!pip install keras-tuner\n",
    "!pip install print_schema\n",
    "!pip install pydub\n",
    "!pip install opensmile\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
    "!pip install transformers\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6458d2b0-349a-468f-a520-cc7efc28b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31bfc4c-1b1d-4923-af2b-2fb3eeb8446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "class ADdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, archs,dropout,freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 256, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        \n",
    "        layers = []          \n",
    "        layers.append(nn.Linear(D_in, archs[0]))\n",
    "        layers.append(nn.BatchNorm1d(num_features=archs[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "          \n",
    "        for i in range(1,len(archs)):\n",
    "          layers.append(nn.Linear(archs[i-1], archs[i]))\n",
    "          layers.append(nn.BatchNorm1d(num_features=archs[i]))\n",
    "          layers.append(nn.ReLU())\n",
    "          layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(archs[-1], 1))\n",
    "        self.classifier  = nn.Sequential(*layers)\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(number_of_layers-1):\n",
    "          layers.append(nn.Linear(fc_layer_size, fc_layer_size))\n",
    "          layers.append(nn.BatchNorm1d(num_features=fc_layer_size))\n",
    "          layers.append(nn.ReLU())\n",
    "          layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(fc_layer_size, 1))\n",
    "        self.classifier  = nn.Sequential(*layers)\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "\n",
    "            nn.Linear(D_in, fc_layer_size),\n",
    "            nn.BatchNorm1d(num_features=fc_layer_size), ## kk\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_layer_size, 1),\n",
    "            #nn.BatchNorm1d(num_features=128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, 1)\n",
    "            \n",
    "            #nn.BatchNorm1d(num_features=128), ###\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Dropout(0.5),\n",
    "\n",
    "            #nn.Linear(H, D_out)\n",
    "            #nn.Linear(128, 2)\n",
    "            #nn.Linear(128, 1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "def initialize_model(archs,dropout,train_loader,epochs):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(archs,dropout,freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn =  torch.nn.BCEWithLogitsLoss()\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train_epoch(model, train_loader,val_loader, optim,scheduler):\n",
    "      model.train()\n",
    "      # For each batch of training data...\n",
    "      train_loss_sum = 0\n",
    "      train_accuracy_epoch = 0\n",
    "      for batch in (train_loader):\n",
    "          optim.zero_grad()\n",
    "          input_ids = batch['input_ids'].to(device)\n",
    "          attention_mask = batch['attention_mask'].to(device)\n",
    "          labels = batch['labels'].to(device)\n",
    "\n",
    "          # Perform a forward pass. This will return logits.\n",
    "          logits = model(input_ids, attention_mask)\n",
    "\n",
    "          # Compute loss and accumulate the loss values\n",
    "          logits = logits.reshape(-1) #silebilirsin\n",
    "          \n",
    "          loss = loss_fn(logits, labels.float())\n",
    "          train_loss_sum += loss.item()\n",
    "\n",
    "          logits_class = logits > 0.5\n",
    "\n",
    "          train_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "          train_accuracy_epoch += train_acc\n",
    "\n",
    "          # Perform a backward pass to calculate gradients\n",
    "          loss.backward()\n",
    "          # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "          # Update parameters and the learning rate\n",
    "          optim.step()\n",
    "          scheduler.step()\n",
    "          #wandb.log({\"batch loss\": loss.item()})\n",
    "\n",
    "      model.eval()\n",
    "      # Tracking variables\n",
    "      val_loss_sum = 0\n",
    "      val_accuracy_epoch = 0\n",
    "      # For each batch in our validation set...\n",
    "      for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "          logits = model(input_ids, attention_mask)\n",
    "\n",
    "          # Compute loss\n",
    "          logits = logits.reshape(-1)\n",
    "          loss = loss_fn(logits, labels.float())\n",
    "          val_loss_sum +=loss.item()\n",
    "\n",
    "          logits_class = logits > 0.5\n",
    "          val_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "          val_accuracy_epoch += val_acc\n",
    "\n",
    "\n",
    "      return model,(train_loss_sum / len(train_loader)),(train_accuracy_epoch/len(train_loader)),(val_loss_sum / len(val_loader)),(val_accuracy_epoch/len(val_loader))\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optim, val_loader=None, epochs=50, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = [] \n",
    "    for epoch_i in range(epochs):\n",
    "        train_loss_sum = 0\n",
    "        train_accuracy_epoch = 0\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for batch in (train_loader):\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            logits = logits.reshape(-1) #silebilirsin\n",
    "            \n",
    "            loss = loss_fn(logits, labels.float())\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "            logits_class = logits > 0.5\n",
    "            train_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "            train_accuracy_epoch += train_acc\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = np.round(train_loss_sum/len(train_loader),2)\n",
    "        avg_train_acc = np.round(train_accuracy_epoch/len(train_loader),2)\n",
    "\n",
    "        if evaluation == True:\n",
    "            avg_val_loss, avg_val_acc = evaluate(model, val_loader)\n",
    "        print('Epoch {}, train loss {} , val loss is {}, train acc is {}, val acc is {} '.format(epoch_i,avg_train_loss,avg_val_loss,avg_train_acc,avg_val_acc))\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(avg_val_acc)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model,train_loss_list,val_loss_list,train_acc_list,val_acc_list\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_loss_sum = 0\n",
    "    val_accuracy_epoch = 0\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        val_loss_sum +=loss.item()\n",
    "        avg_val_loss = np.round(val_loss_sum/len(val_dataloader),2)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        val_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        val_accuracy_epoch += val_acc\n",
    "        avg_val_acc = np.round(val_accuracy_epoch/len(val_dataloader),2)\n",
    "\n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "\n",
    "def evaluate_test(model, test_dataloader):\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      \n",
    "      # Compute logits\n",
    "      with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "        # Compute loss\n",
    "        logits = logits.reshape(-1)\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        test_loss_sum +=loss.item()\n",
    "        avg_test_loss = np.round(test_loss_sum/len(test_dataloader),5)\n",
    "\n",
    "        logits_class = logits > 0.5\n",
    "        predictions.append(logits.cpu())\n",
    "        test_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
    "        test_accuracy_epoch += test_acc\n",
    "        avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_loss, avg_test_acc, predictions, labels_list\n",
    "\n",
    "\n",
    "def evaluate_ensemble(models, test_dataloader):\n",
    "    test_loss_sum = 0\n",
    "    test_accuracy_epoch = 0\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "    for batch in test_dataloader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "\n",
    "      prediction = []\n",
    "      for model in models:\n",
    "        model.eval()\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "          logits = model(input_ids, attention_mask)\n",
    "          labels_list.append(labels.cpu())\n",
    "\n",
    "          logits = logits.reshape(-1)\n",
    "          logits_class = logits > 0.5\n",
    "          prediction.append(logits_class)\n",
    "\n",
    "      prediction_ensemble = sum(prediction) > 0.5*len(prediction)\n",
    "      predictions.append(prediction_ensemble.cpu())\n",
    "\n",
    "      test_acc = (labels == prediction_ensemble).sum().item() / labels.size(0)\n",
    "      test_accuracy_epoch += test_acc\n",
    "      avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
    "\n",
    "    return avg_test_acc, predictions, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c531fff-e7b6-4c97-90c6-8ed0f9fd5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4824965f-35af-4b3b-beb4-822ee8725e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8415e329-95c4-46ff-a761-90fbebc16855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/test.csv\")\n",
    "df[\"Content\"].fillna(\"\",inplace=True)\n",
    "test_texts = list(df.loc[:,\"Content\"])\n",
    "test_labels = list(df.loc[:,\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "501cb932-3457-480c-86db-c3515fc6a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "test_dataset = ADdataset(test_encodings, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715e8a2b-bed9-4d4d-9654-12b66cd5e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca62392a-281e-41d4-b352-409ceb78ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "filenames = glob(\"weights/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa64802c-0086-4092-8363-f866b17b8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:57<00:00, 53.73s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(0,100,10)):\n",
    "    model = []\n",
    "    for filename in filenames:\n",
    "        seed = filename.split(\"seed-\")[-1].split(\"_\")[0]\n",
    "        # index = filename.split(\"index-\")[-1].split(\"_\")[0]\n",
    "        if int(seed) == i:\n",
    "            model.append(torch.load(filename))\n",
    "            if len(model) == 10:\n",
    "                break\n",
    "    # print(len(model))\n",
    "    avg_test_acc, predictions, labels_list = evaluate_ensemble(model,test_loader)\n",
    "    results.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fc8eaf3-5aa2-4c0f-88eb-48b65d11eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81786\n",
      "0.88393\n",
      "0.855002\n",
      "0.01967152195433795\n"
     ]
    }
   ],
   "source": [
    "print(min(results))\n",
    "print(max(results))\n",
    "print(np.mean(results))\n",
    "print(np.std(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f9d37dd-309f-4da6-a660-3b1fe0b102a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72b1fcd3-c08e-4d88-b346-6270694f4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:22<00:00, 47.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,4)):\n",
    "    model = []\n",
    "    for filename in filenames:\n",
    "        # seed = filename.split(\"seed-\")[-1].split(\"_\")[0]\n",
    "        index = filename.split(\"index-\")[-1].split(\"_\")[0]\n",
    "        if int(index) == i:\n",
    "            model.append(torch.load(filename))\n",
    "            if len(model) == 10:\n",
    "                break\n",
    "    # print(len(model))\n",
    "    avg_test_acc, predictions, labels_list = evaluate_ensemble(model,test_loader)\n",
    "    index_results.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9e714a9-9657-4a17-a910-e8974ca5dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:39<00:00, 53.02s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,4)):\n",
    "    model = []\n",
    "    for filename in reversed(filenames):\n",
    "        # seed = filename.split(\"seed-\")[-1].split(\"_\")[0]\n",
    "        index = filename.split(\"index-\")[-1].split(\"_\")[0]\n",
    "        if int(index) == i:\n",
    "            model.append(torch.load(filename))\n",
    "            if len(model) == 10:\n",
    "                break\n",
    "    # print(len(model))\n",
    "    avg_test_acc, predictions, labels_list = evaluate_ensemble(model,test_loader)\n",
    "    index_results.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbfd1f8c-f173-4bdb-9305-76f345288601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85536\n",
      "0.88393\n",
      "0.8744066666666667\n",
      "0.013468027158999767\n"
     ]
    }
   ],
   "source": [
    "print(min(index_results))\n",
    "print(max(index_results))\n",
    "print(np.mean(index_results))\n",
    "print(np.std(index_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a44865f-cd90-43e7-904c-07fc46dc1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84286\n",
      "0.88393\n",
      "0.8675616666666667\n",
      "0.016890335912059937\n"
     ]
    }
   ],
   "source": [
    "print(min(index_results))\n",
    "print(max(index_results))\n",
    "print(np.mean(index_results))\n",
    "print(np.std(index_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1e198dc-4cc2-4c5a-a9d4-2e8fc89a0da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [23:50<00:00, 143.02s/it]\n"
     ]
    }
   ],
   "source": [
    "seed_results = []\n",
    "for i in tqdm(range(0,100,10)):\n",
    "    model = []\n",
    "    for filename in filenames:\n",
    "        seed = filename.split(\"seed-\")[-1].split(\"_\")[0]\n",
    "        # index = filename.split(\"index-\")[-1].split(\"_\")[0]\n",
    "        if int(seed) == i:\n",
    "            model.append(torch.load(filename))\n",
    "    # print(len(model))\n",
    "    avg_test_acc, predictions, labels_list = evaluate_ensemble(model,test_loader)\n",
    "    seed_results.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08510577-49ac-4aa4-8f44-b85b14211729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84643\n",
      "0.88393\n",
      "0.873573\n",
      "0.012000293371413881\n"
     ]
    }
   ],
   "source": [
    "print(min(seed_results))\n",
    "print(max(seed_results))\n",
    "print(np.mean(seed_results))\n",
    "print(np.std(seed_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb27c9b-7847-4543-bc29-744785f4abf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
